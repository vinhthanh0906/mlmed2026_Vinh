\documentclass[conference]{IEEEtran}
\usepackage{graphicx} 
\usepackage{booktabs}
\bibliography{references}


\title{Machine Learning in Medicine: Pratice 1 \par (Heartbeat type Classification )}
\author{Vinh Thanh Nguyen}
\date{January 2026}

\begin{document}

\maketitle

\section{Introduction}
Machine Learning nowadays provide many method that help many field: Finance \& Stock, Math, and so on. Especically, Machine Learning play the critical role in medical field, the most common task that Machine Learning does is \textbf{Diagnosis decease}. The input will be symptoms, health state, measurement of body (weight, blood pressure, height, etc.). This implementation of Machine Learning save a significant amount of times for doctors and prevent any human faults. 

\section{Abstract}
Heartbeat type test is one the most important stage to measure and health assessment. The result of the test in the most of times will be the class of your heartbeat. In this experiment, the outputs are classes from 1-5. Input is the ECG measurement statistics, from this doctor can easily evaluate your health condition.

\section{Dataset Overview}
This dataset has been used in exploring heartbeat classification. The signals correspond to electrocardiogram (ECG) shapes of heartbeats for the normal case and the cases affected by different arrhythmias and myocardial infarction. These signals are preprocessed and segmented, with each segment corresponding to a heartbeat.
\section*{Content}

\subsection*{Arrhythmia Dataset}

\begin{itemize}
    \item \textbf{Number of Samples}: 109{,}446
    \item \textbf{Number of Categories}: 5
    \item \textbf{Sampling Frequency}: 125 Hz
    \item \textbf{Data Source}: PhysioNet's MIT-BIH Arrhythmia Dataset
    \item \textbf{Classes}: \texttt{['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]}
\end{itemize}

\subsection*{The PTB Diagnostic ECG Database}

\begin{itemize}
    \item \textbf{Number of Samples}: 14{,}552
    \item \textbf{Number of Categories}: 2
    \item \textbf{Sampling Frequency}: 125 Hz
    \item \textbf{Data Source}: PhysioNet's PTB Diagnostic Database
\end{itemize}

\section{Visualization}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Screenshot 2026-01-20 at 09.21.46.png}
    \caption{Class distribution}
    \label{fig:placeholder}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Screenshot 2026-01-20 at 09.25.56.png}
    \caption{Output Distribution}
    \label{fig:placeholder}
\end{figure}

\newpage
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{dist_5.png}
    \caption{Random 5 columns sample distribution }
    \label{fig:long_figure}
\end{figure*}

The classes outputs distribution are not balance, \textbf{Class 0} has dominate appearance, which can cause imbalance data, a common cause lead to poor prediction model performance.

But the advantage of this dataset is every classes have been well clustered which ideal for almost every type of cluster-based models like KNN, SVM, .etc



\section{Experiment: SVM and KNN}
In this experiment, we consider two scenarios: \textit{imbalanced-data usage} and
\textit{balanced-data usage},in order to analyze the performance differences and
highlight the importance of the data preprocessing stage. In particular, this
experiment demonstrates the limitations of directly applying machine learning
models to raw, imbalanced data and emphasizes the necessity of preprocessing
techniques such as resampling for improving minority-class recognition.
\vspace{1em}


% ---------------- SVM ----------------
\subsection{SVM: Imbalanced Data}

\begin{table}[h]
\centering
\caption{Classification Performance of SVC on Imbalanced Data (cache\_size = 500)}
\label{tab:svc_report_full}
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\midrule
0 & 0.9672 & 0.9982 & 0.9825 & 18{,}118 \\
1 & 0.9632 & 0.5647 & 0.7120 & 556 \\
2 & 0.9667 & 0.8626 & 0.9117 & 1{,}448 \\
3 & 0.7500 & 0.4815 & 0.5865 & 162 \\
4 & 0.9959 & 0.9111 & 0.9516 & 1{,}608 \\
\midrule
Accuracy &  &  & \textbf{0.9680} & 21{,}892 \\
\midrule
Macro Avg & 0.9286 & 0.7636 & 0.8289 & 21{,}892 \\
Weighted Avg & 0.9676 & 0.9680 & 0.9657 & 21{,}892 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{KNN: Imbalanced Data}

\begin{table}[h]
\centering
\caption{Classification Performance of kNN on Imbalanced Data}
\label{tab:knn_report_full}
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\midrule
0 & 0.9777 & 0.9946 & 0.9861 & 18{,}118 \\
1 & 0.8970 & 0.6421 & 0.7484 & 556 \\
2 & 0.9395 & 0.9012 & 0.9200 & 1{,}448 \\
3 & 0.7630 & 0.6358 & 0.6936 & 162 \\
4 & 0.9941 & 0.9509 & 0.9720 & 1{,}608 \\
\midrule
Accuracy &  &  & \textbf{0.9736} & 21{,}892 \\
\midrule
Macro Avg & 0.9143 & 0.8249 & 0.8640 & 21{,}892 \\
Weighted Avg & 0.9727 & 0.9736 & 0.9725 & 21{,}892 \\
\bottomrule
\end{tabular}
\end{table}


\newpage
\section{Summary}

We can see that the precision and other metrics result are quite good as expected, class 1 , class 2 and 4 has higher precision , recall , F1 than other class. This happened because the number of samples belonging to those classes is much more than the others, so the model learns and biases for those classes. In an intuitive way, the more data of the class that feed into the model, the better prediction the model perform. Next time we will test with balanced data.



\begin{thebibliography}{99}

\bibitem{svm}
C. Cortes and V. Vapnik,
``Support-vector networks,''
\textit{Machine Learning}, vol. 20, pp. 273--297, 1995.


\bibitem{heartbeat_dataset}
ECG Heartbeat Categorization Dataset, Kaggle, Available: \url{https://www.kaggle.com/datasets/shayanfazeli/heartbeat}. Accessed: 2026-01-XX. Dataset composed of segmented/preprocessed ECG signals for heartbeat classification derived from the MIT-BIH Arrhythmia Dataset and PTB Diagnostic ECG Database. :contentReference[oaicite:0]{index=0}

\bibitem{sklearn_svm}
Scikit-Learn Documentation, ``Support Vector Machines,'' Available: \url{https://scikit-learn.org/stable/modules/svm.html}. Accessed: 2026-01-XX. SVM methods are supervised learning models for classification, regression, and outlier detection, with kernel functions defining decision boundaries. :contentReference[oaicite:1]{index=1}

\end{thebibliography}




\end{document}
